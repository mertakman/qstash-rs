use crate::errors::QstashError;
use serde::{Deserialize, Serialize};

#[derive(Serialize, Deserialize, Debug)]
pub struct ChatCompletionRequest {
    /// Name of the model.
    pub model: String,

    /// One or more chat messages.
    pub messages: Vec<Message>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim.
    pub frequency_penalty: Option<f64>,

    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
    pub logit_bias: Option<std::collections::HashMap<String, f64>>,

    /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
    pub logprobs: Option<bool>,

    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to true if this parameter is used.
    pub top_logprobs: Option<u8>,

    /// The maximum number of tokens that can be generated in the chat completion.
    pub max_tokens: Option<u32>,

    /// How many chat completion choices to generate for each input message.
    pub n: Option<u8>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics.
    pub presence_penalty: Option<f64>,

    /// An object specifying the format that the model must output.
    pub response_format: Option<ResponseFormat>,

    /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
    pub seed: Option<u64>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    pub stop: Option<Vec<String>>,

    /// If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
    pub stream: Option<bool>,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    pub temperature: Option<f64>,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass.
    pub top_p: Option<f64>,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct Message {
    /// The role of the message author. One of `system`, `assistant`, or `user`.
    pub role: String,

    /// The content of the message.
    pub content: String,

    /// An optional name for the participant. Provides the model information to differentiate between participants of the same role.
    pub name: Option<String>,
}

#[derive(Serialize, Deserialize, Debug)]
#[serde(rename_all = "snake_case")]
pub enum FormatType {
    Text,
    JsonObject,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct ResponseFormat {
    /// Must be one of `text` or `json_object`.
    #[serde(rename = "type")]
    pub format_type: FormatType,
}

pub enum ChatCompletionResponse {
    Stream(StreamResponse),
    Direct(DirectResponse),
}

#[derive(Debug, Serialize, Deserialize)]
pub struct DirectResponse {
    // A unique identifier for the chat completion
    pub id: String,
    // A list of chat completion choices. Can be more than one if n is greater than 1
    pub choices: Vec<Choice>,
    // The Unix timestamp (in seconds) of when the chat completion was created
    pub created: i64,
    // The model used for the chat completion
    pub model: String,
    // This fingerprint represents the backend configuration that the model runs with
    pub system_fingerprint: String,
    // The object type, which is always "chat.completion"
    pub object: String,
    // Usage statistics for the completion request
    pub usage: Usage,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Choice {
    // A chat completion message generated by the model
    pub message: Message,
    // The reason the model stopped generating tokens
    #[serde(rename = "finishReason")]
    pub finish_reason: String,
    // The stop string or token id that caused the completion to stop
    #[serde(rename = "stopReason")]
    pub stop_reason: Option<String>,
    // The index of the choice in the list of choices
    pub index: i32,
    // Log probability information for the choice
    pub logprobs: Option<LogProbs>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct LogProbs {
    // A list of message content tokens with log probability information
    pub content: Vec<TokenInfo>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TokenInfo {
    // The token
    pub token: String,
    // The log probability of this token
    pub logprob: f64,
    // A list of integers representing the UTF-8 bytes representation of the token
    pub bytes: Option<Vec<i32>>,
    // List of the most likely tokens and their log probability
    pub top_logprobs: Vec<TopLogProb>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct TopLogProb {
    // The token
    pub token: String,
    // The log probability of this token
    pub logprob: f64,
    // A list of integers representing the UTF-8 bytes representation of the token
    pub bytes: Option<Vec<i32>>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Usage {
    // Number of tokens in the generated completion
    pub completion_tokens: i32,
    // Number of tokens in the prompt
    pub prompt_tokens: i32,
    // Total number of tokens used in the request (prompt + completion)
    pub total_tokens: i32,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StreamMessage {
    // A unique identifier for the chat completion. Each chunk has the same ID
    pub id: String,
    // A list of chat completion choices. Can be more than one if n is greater than 1. Can also be empty for the last chunk
    pub choices: Vec<StreamChoice>,
    // The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp
    pub created: i64,
    // The model used for the chat completion
    pub model: String,
    // This fingerprint represents the backend configuration that the model runs with
    pub system_fingerprint: String,
    // The object type, which is always "chat.completion.chunk"
    pub object: String,
    // Contains a null value except for the last chunk which contains the token usage statistics for the entire request
    pub usage: Option<Usage>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StreamChoice {
    // A chat completion delta generated by streamed model responses
    pub delta: Delta,
    // The reason the model stopped generating tokens
    pub finish_reason: Option<String>,
    // The index of the choice in the list of choices
    pub index: i32,
    // Log probability information for the choice
    pub logprobs: Option<LogProbs>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Delta {
    // The role of the author of this message
    pub role: Option<String>,
    // The contents of the chunk message
    pub content: Option<String>,
}

enum ChunkType {
    Message(Vec<u8>),
    Done(),
}

pub struct StreamResponse {
    response: Option<reqwest::Response>, // Use RefCell for interior mutability
    buffer: Vec<u8>,
}

impl StreamResponse {
    pub fn new(response: reqwest::Response) -> Self {
        Self {
            response: Some(response),
            buffer: Vec::new(),
        }
    }

    pub fn default() -> Self {
        Self {
            response: None,
            buffer: Vec::new(),
        }
    }

    pub async fn get_next_stream_message(&mut self) -> Result<Option<StreamMessage>, QstashError> {
        let chunk = self.poll_chunk().await?;
        match chunk {
            ChunkType::Message(data) => {
                let message =
                    serde_json::from_slice(&data).map_err(QstashError::ResponseStreamParseError)?;
                Ok(Some(message))
            }
            ChunkType::Done() => Ok(None),
        }
    }

    async fn poll_chunk(&mut self) -> Result<ChunkType, QstashError> {
        loop {
            let response = match &mut self.response {
                Some(r) => r,
                None => return Ok(ChunkType::Done()),
            };

            // Get the next chunk
            let chunk = match response.chunk().await.map_err(QstashError::RequestFailed)? {
                Some(c) => c,
                None => return Ok(ChunkType::Done()),
            };
            // Now we can mutably borrow self for extract_next_message
            if let Some(message) = self.extract_next_message(&chunk.to_vec()) {
                match message.as_slice() {
                    b"[DONE]" => {
                        self.response = None;
                        return Ok(ChunkType::Done());
                    }
                    _ => return Ok(ChunkType::Message(message)),
                }
            }
        }
    }

    // Takes a chunk of bytes and returns a complete message if available
    fn extract_next_message(&mut self, chunk: &Vec<u8>) -> Option<Vec<u8>> {
        // Append new chunk to existing buffer
        self.buffer.extend_from_slice(chunk);

        // Look for delimiter
        if let Some(msg_end) = self.buffer.windows(2).position(|w| w == b"\n\n") {
            // Extract the message (excluding delimiter)
            let message = self.buffer[..msg_end].to_vec();

            // Remove the processed message and delimiter from buffer
            self.buffer = self.buffer[msg_end + 2..].to_vec();

            Some(message)
        } else {
            None
        }
    }
}

#[cfg(test)]
mod tests {
    use crate::rate_limited_client::RateLimitedClient;

    use super::*;
    use httpmock::prelude::*;
    use reqwest::{Method, Url};

    #[tokio::test]
    async fn test_send_request_success() {
        // Arrange
        let server = MockServer::start_async().await;
        let mock = server.mock(|when, then| {
            when.method(GET).path("/test");
            then.status(200);
        });

        let client = RateLimitedClient::new("test_api_key".to_string());
        let url = Url::parse(&format!("{}/test", &server.base_url())).unwrap();
        let request_builder = client.get_request_builder(Method::GET, url);

        // Act
        let result = client.send_request(request_builder).await;

        // Assert
        assert!(result.is_ok());
        mock.assert();
    }

    #[tokio::test]
    async fn test_send_request_daily_rate_limit_exceeded() {
        // Arrange
        let server = MockServer::start_async().await;
        let mock = server.mock(|when, then| {
            when.method(GET).path("/test");
            then.status(429)
                .header("RateLimit-Limit", "1000")
                .header("RateLimit-Reset", "3600");
        });

        let client = RateLimitedClient::new("test_api_key".to_string());
        let url = Url::parse(&format!("{}/test", &server.base_url())).unwrap();
        let request_builder = client.get_request_builder(Method::GET, url);

        // Act
        let result = client.send_request(request_builder).await;

        // Assert
        match result {
            Err(QstashError::DailyRateLimitExceeded { reset }) => assert_eq!(reset, 3600),
            _ => panic!("Expected DailyRateLimitExceeded error"),
        }
        mock.assert();
    }

    #[tokio::test]
    async fn test_send_request_burst_rate_limit_exceeded() {
        // Arrange
        let server = MockServer::start_async().await;
        let mock = server.mock(|when, then| {
            when.method(GET).path("/test");
            then.status(429)
                .header("Burst-RateLimit-Limit", "100")
                .header("Burst-RateLimit-Reset", "60");
        });

        let client = RateLimitedClient::new("test_api_key".to_string());
        let url = Url::parse(&format!("{}/test", &server.base_url())).unwrap();
        let request_builder = client.get_request_builder(Method::GET, url);

        // Act
        let result = client.send_request(request_builder).await;

        // Assert
        match result {
            Err(QstashError::BurstRateLimitExceeded { reset }) => assert_eq!(reset, 60),
            _ => panic!("Expected BurstRateLimitExceeded error"),
        }
        mock.assert();
    }

    #[tokio::test]
    async fn test_send_request_chat_rate_limit_exceeded() {
        // Arrange
        let server = MockServer::start_async().await;
        let mock = server.mock(|when, then| {
            when.method(GET).path("/test");
            then.status(429)
                .header("x-ratelimit-limit-requests", "100")
                .header("x-ratelimit-reset-requests", "30")
                .header("x-ratelimit-reset-tokens", "45");
        });

        let client = RateLimitedClient::new("test_api_key".to_string());
        let url = Url::parse(&format!("{}/test", &server.base_url())).unwrap();
        let request_builder = client.get_request_builder(Method::GET, url);

        // Act
        let result = client.send_request(request_builder).await;

        // Assert
        match result {
            Err(QstashError::ChatRateLimitExceeded {
                reset_requests,
                reset_tokens,
            }) => {
                assert_eq!(reset_requests, 30);
                assert_eq!(reset_tokens, 45);
            }
            _ => panic!("Expected ChatRateLimitExceeded error"),
        }
        mock.assert();
    }

    #[tokio::test]
    async fn test_send_request_unspecified_rate_limit_exceeded() {
        // Arrange
        let server = MockServer::start_async().await;
        let mock = server.mock(|when, then| {
            when.method(GET).path("/test");
            then.status(429);
        });

        let client = RateLimitedClient::new("test_api_key".to_string());
        let url = Url::parse(&format!("{}/test", &server.base_url())).unwrap();
        let request_builder = client.get_request_builder(Method::GET, url);

        // Act
        let result = client.send_request(request_builder).await;

        // Assert
        match result {
            Err(QstashError::UnspecifiedRateLimitExceeded) => (),
            _ => panic!("Expected UnspecifiedRateLimitExceeded error"),
        }
        mock.assert();
    }
}
