use crate::errors::QstashError;
use serde::{Deserialize, Serialize};

#[derive(Debug, Default, Deserialize, Serialize)]
#[serde(default)]
pub struct ChatCompletionRequest {
    /// Name of the model.
    pub model: String,

    /// One or more chat messages.
    pub messages: Vec<Message>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim.
    pub frequency_penalty: Option<f64>,

    /// Modify the likelihood of specified tokens appearing in the completion.
    /// Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
    pub logit_bias: Option<std::collections::HashMap<String, f64>>,

    /// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.
    pub logprobs: Option<bool>,

    /// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to true if this parameter is used.
    pub top_logprobs: Option<u8>,

    /// The maximum number of tokens that can be generated in the chat completion.
    pub max_tokens: Option<u32>,

    /// How many chat completion choices to generate for each input message.
    pub n: Option<u8>,

    /// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model’s likelihood to talk about new topics.
    pub presence_penalty: Option<f64>,

    /// An object specifying the format that the model must output.
    pub response_format: Option<ResponseFormat>,

    /// This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result.
    pub seed: Option<u64>,

    /// Up to 4 sequences where the API will stop generating further tokens.
    pub stop: Option<Vec<String>>,

    /// If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a data: [DONE] message.
    pub stream: Option<bool>,

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
    pub temperature: Option<f64>,

    /// An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass.
    pub top_p: Option<f64>,
}

#[derive(Debug, Default, Deserialize, Serialize, PartialEq)]
#[serde(default)]
pub struct Message {
    /// The role of the message author. One of `system`, `assistant`, or `user`.
    pub role: String,

    /// The content of the message.
    pub content: String,

    /// An optional name for the participant. Provides the model information to differentiate between participants of the same role.
    pub name: Option<String>,
}

#[derive(Serialize, Deserialize, Debug)]
#[serde(rename_all = "snake_case")]
pub enum FormatType {
    Text,
    JsonObject,
}

#[derive(Serialize, Deserialize, Debug)]
pub struct ResponseFormat {
    /// Must be one of `text` or `json_object`.
    #[serde(rename = "type")]
    pub format_type: FormatType,
}

#[derive(Debug)]
pub enum ChatCompletionResponse {
    Stream(StreamResponse),
    Direct(DirectResponse),
}

#[derive(Debug, Default, Serialize, Deserialize, PartialEq)]
#[serde(default)]
pub struct DirectResponse {
    // A unique identifier for the chat completion
    pub id: String,
    // A list of chat completion choices. Can be more than one if n is greater than 1
    pub choices: Vec<Choice>,
    // The Unix timestamp (in seconds) of when the chat completion was created
    pub created: i64,
    // The model used for the chat completion
    pub model: String,
    // This fingerprint represents the backend configuration that the model runs with
    pub system_fingerprint: String,
    // The object type, which is always "chat.completion"
    pub object: String,
    // Usage statistics for the completion request
    pub usage: Usage,
}

#[derive(Debug, Default, Serialize, Deserialize, PartialEq)]
pub struct Choice {
    // A chat completion message generated by the model
    pub message: Message,
    // The reason the model stopped generating tokens
    #[serde(rename = "finishReason")]
    pub finish_reason: Option<String>,
    // The stop string or token id that caused the completion to stop
    #[serde(rename = "stopReason")]
    pub stop_reason: Option<String>,
    // The index of the choice in the list of choices
    pub index: i32,
    // Log probability information for the choice
    pub logprobs: Option<LogProbs>,
}

#[derive(Debug, Serialize, Deserialize, PartialEq)]
pub struct LogProbs {
    // A list of message content tokens with log probability information
    pub content: Vec<TokenInfo>,
}

#[derive(Debug, Serialize, Deserialize, PartialEq)]
pub struct TokenInfo {
    // The token
    pub token: String,
    // The log probability of this token
    pub logprob: f64,
    // A list of integers representing the UTF-8 bytes representation of the token
    pub bytes: Option<Vec<i32>>,
    // List of the most likely tokens and their log probability
    pub top_logprobs: Vec<TopLogProb>,
}

#[derive(Debug, Serialize, Deserialize, PartialEq)]
pub struct TopLogProb {
    // The token
    pub token: String,
    // The log probability of this token
    pub logprob: f64,
    // A list of integers representing the UTF-8 bytes representation of the token
    pub bytes: Option<Vec<i32>>,
}

#[derive(Debug, Default, Serialize, Deserialize, PartialEq)]
#[serde(default)]
pub struct Usage {
    // Number of tokens in the generated completion
    pub completion_tokens: i32,
    // Number of tokens in the prompt
    pub prompt_tokens: i32,
    // Total number of tokens used in the request (prompt + completion)
    pub total_tokens: i32,
}

#[derive(Debug, Default, Serialize, Deserialize)]
#[serde(default)]
pub struct StreamMessage {
    // A unique identifier for the chat completion. Each chunk has the same ID
    pub id: String,
    // A list of chat completion choices. Can be more than one if n is greater than 1. Can also be empty for the last chunk
    pub choices: Vec<StreamChoice>,
    // The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp
    pub created: i64,
    // The model used for the chat completion
    pub model: String,
    // This fingerprint represents the backend configuration that the model runs with
    pub system_fingerprint: String,
    // The object type, which is always "chat.completion.chunk"
    pub object: String,
    // Contains a null value except for the last chunk which contains the token usage statistics for the entire request
    pub usage: Option<Usage>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct StreamChoice {
    // A chat completion delta generated by streamed model responses
    pub delta: Delta,
    // The reason the model stopped generating tokens
    pub finish_reason: Option<String>,
    // The index of the choice in the list of choices
    pub index: i32,
    // Log probability information for the choice
    pub logprobs: Option<LogProbs>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct Delta {
    // The role of the author of this message
    pub role: Option<String>,
    // The contents of the chunk message
    pub content: Option<String>,
}

enum ChunkType {
    Message(Vec<u8>),
    Done(),
}

#[derive(Debug)]
pub struct StreamResponse {
    response: Option<reqwest::Response>, // Use RefCell for interior mutability
    buffer: Vec<u8>,
}

impl StreamResponse {
    pub fn new(response: reqwest::Response) -> Self {
        Self {
            response: Some(response),
            buffer: Vec::new(),
        }
    }

    pub fn default() -> Self {
        Self {
            response: None,
            buffer: Vec::new(),
        }
    }

    pub async fn get_next_stream_message(&mut self) -> Result<Option<StreamMessage>, QstashError> {
        let chunk = self.poll_chunk().await?;
        match chunk {
            ChunkType::Message(data) => {
                let message =
                    serde_json::from_slice(&data).map_err(QstashError::ResponseStreamParseError)?;
                Ok(Some(message))
            }
            ChunkType::Done() => Ok(None),
        }
    }

    async fn poll_chunk(&mut self) -> Result<ChunkType, QstashError> {
        loop {
            // Now we can mutably borrow self for extract_next_message
            if let Some(message) = self.extract_next_message() {
                match message.as_slice() {
                    b"[DONE]" => {
                        self.response = None;
                        return Ok(ChunkType::Done());
                    }
                    _ => return Ok(ChunkType::Message(message)),
                }
            }

            let response = match &mut self.response {
                Some(r) => r,
                None => return Ok(ChunkType::Done()),
            };

            // Get the next chunk
            let chunk = match response.chunk().await.map_err(QstashError::RequestFailed)? {
                Some(c) => c,
                None => return Ok(ChunkType::Done()),
            };

            self.buffer.extend_from_slice(&chunk);
        }
    }

    // Takes a chunk of bytes and returns a complete message if available
    fn extract_next_message(&mut self) -> Option<Vec<u8>> {
        if self.buffer == b"[DONE]" {
            self.buffer.clear();
            return Some(b"[DONE]".into());
        }
        // Look for delimiter
        if let Some(msg_end) = self.buffer.windows(2).position(|w| w == b"\n\n") {
            // Extract the message (excluding delimiter)
            let message = self.buffer[..msg_end].to_vec();
            // Remove the processed message and delimiter from buffer
            self.buffer = self.buffer[msg_end + 2..].to_vec();

            Some(message)
        } else {
            None
        }
    }
}

#[cfg(test)]
mod tests {
    use crate::llm_types::StreamResponse;
    #[test]
    fn test_extract_next_message_logic() {
        let mut stream_response = StreamResponse::default();
        let chunk1 = b"{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1625097600,\"model\":\"gpt-4\",\"choices\":[{\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}]}\n".to_vec();
        let chunk2 = b"\n{\"id\":\"chatcmpl-123\",\"object\":\"chat.completion.chunk\",\"created\":1625097600,\"model\":\"gpt-4\",\"choices\":[{\"delta\":{\"content\":\" World\"},\"finish_reason\":null,\"index\":0,\"logprobs\":null}]}\n\n".to_vec();
        let chunk3 = b"[DONE]".to_vec();

        stream_response.buffer.extend_from_slice(&chunk1);
        assert!(stream_response.extract_next_message().is_none());
        stream_response.buffer.extend_from_slice(&chunk2);
        assert!(stream_response.extract_next_message().is_some());
        assert!(stream_response.extract_next_message().is_some());

        stream_response.buffer.extend_from_slice(&chunk3);
        assert_eq!(
            stream_response.extract_next_message(),
            Some(b"[DONE]".to_vec())
        );
    }
}
